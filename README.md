
# ğŸ§  Consensus Algorithm Demonstrator (Paxos / Raft Model)

## ğŸ“˜ Overview

The **Consensus Algorithm Demonstrator** is a Python-based simulation of two fundamental distributed consensus algorithms â€” **Paxos** and **Raft**.
These algorithms ensure that multiple distributed nodes can **agree on a single value**, even if some nodes fail or messages are delayed.

This project helps visualize and understand **how leader election, log replication, and fault tolerance** are achieved in distributed systems.

---

## âš™ï¸ Features

* âœ… Implements **Paxos Algorithm** (Proposer, Acceptor, Learner)
* âœ… Implements **Raft Algorithm** (Leader Election, Log Replication)
* âš™ï¸ Simulates network latency and node communication
* ğŸ“¡ Uses Pythonâ€™s `asyncio` to simulate concurrent node behavior
* ğŸ§© Single file, easy to run â€” no external dependencies

---

## ğŸ–¥ï¸ Technologies Used

* **Language:** Python 3.x
* **Libraries:** `asyncio`, `time`, `random`, `collections`
* **Algorithms:** Paxos, Raft

---

## ğŸ“‚ Project Structure

```
Consensus-Algorithm-Demonstrator/
â”‚
â”œâ”€â”€ paxos_raft_demo.py     # Main simulation file
â””â”€â”€ README.md              # Documentation file
```

---

## ğŸš€ How to Run

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/<your-username>/Consensus-Algorithm-Demonstrator.git
cd Consensus-Algorithm-Demonstrator
```

### 2ï¸âƒ£ Run Paxos Simulation

```bash
python paxos_raft_demo.py paxos
```

### 3ï¸âƒ£ Run Raft Simulation

```bash
python paxos_raft_demo.py raft
```

---

## ğŸ§© Paxos Algorithm Flow

1. **Proposer** sends a prepare request to acceptors.
2. **Acceptors** promise to respond only to higher proposal numbers.
3. **Proposer** sends an accept request with a value.
4. **Learners** learn the chosen value once a majority of acceptors accept it.

ğŸ“ˆ **Sample Output:**

```
[P1] Value chosen: ReleaseWater
[Learner L1] Learned: ReleaseWater
```

---

## âš™ï¸ Raft Algorithm Flow

1. **Nodes** start as followers.
2. **Candidates** start an election after timeout.
3. **Leader** is elected after receiving majority votes.
4. **Leader** replicates log entries to followers.

ğŸ“ˆ **Sample Output:**

```
--- Raft Demo ---
[N2] wins election, becomes leader (term 3)
Leader is N2, sending client commands
[N2] Command committed: SetMode=ALERT
[N2] Command committed: ReleaseResources
```

---

## ğŸ” Key Concepts

| Algorithm | Component | Description                          |
| --------- | --------- | ------------------------------------ |
| Paxos     | Proposer  | Suggests values for consensus        |
| Paxos     | Acceptor  | Votes on proposed values             |
| Paxos     | Learner   | Learns the final agreed value        |
| Raft      | Leader    | Manages replication and coordination |
| Raft      | Follower  | Responds to leader heartbeats        |
| Raft      | Candidate | Competes to become the next leader   |

---

## ğŸ§  Learning Outcomes

* Understand **how distributed systems achieve consensus**
* Visualize **leader election and fault tolerance**
* Learn the difference between **Paxos** and **Raft** algorithms

---

## ğŸ”® Future Enhancements

* Simulate **node failures** and **network partitions**
* Add a **graphical dashboard** to visualize leader elections
* Extend to support **persistent logs and recovery**

---

## ğŸ‘©â€ğŸ’» Author

Akshaya P
Akalya S
Akash S

